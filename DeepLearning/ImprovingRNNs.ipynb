{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving RNNs with GRU and LSTM\n",
    "\n",
    "As we know, standard RNNs have multiple problems including the vanishing/exploding gradient problem. By construction, RNNs can be easily adjusted with custom cells between the time steps, similar to custom layers in standard Neural Networks. Two standard adaptations of standard RNNs to solve some of its problems are the `Gated Recurrent Unit` and the `Long-Short-Term-Memory` which have had a lasting impact on Machine Learning Research to date. We want to see, how we can implement these two variants in PyTorch, similar to how we implement the `BasicRNN`.\n",
    "\n",
    "\n",
    "##### Long-Short-Term-Memory (LSTM) \n",
    "\n",
    "For an overview, see https://en.wikipedia.org/wiki/Long_short-term_memory and the original paper https://www.researchgate.net/publication/13853244_Long_Short-Term_Memory.\n",
    "\n",
    "- idea is to, via an additional LSTM-cell, enforce that the network learns that some things are not worth remembering \n",
    "- somewhat solve Vanishing Gradient Problem, exploding gradients still not solved\n",
    "\n",
    "The above things it enforces via the below LSTM-cell (taken from wikipedia), where $h_t$ is our classical hidden state, $c_t$ is the memory/cell state at time and $x_t$ is our input at time $t$. A key difference is that $h_t$ also is the output of our model at time $t$ as opposed to having a explicit output. \n",
    "\n",
    "\n",
    "<div style=\"width: 50%; margin: 0 auto\">\n",
    "  <img src=\"LSTM_Cell.svg.png\" style=\"width: 100%; height: auto; border: 1px solid #ddd\">\n",
    "</div>\n",
    "\n",
    "\n",
    "Let's look at an example, in the following, we are going to implement a basic LSTM Network combined with a standard linear layer to see how we can also use Recurrent Networks for images, this, often, is not as good as Convolutional Networks, but i think it is still nice to see that we can also solve image classification tasks using a Recurrent Network.\n",
    "\n",
    "To make training a bit faster, we are not going to implement it the same way we did the `RNN` but instead use batching, which makes the code (imo) not as redeable, but way faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision # for dataset of MNIST images\n",
    "from torchvision import transforms\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants we need\n",
    "\n",
    "input_dim = 28 # we take every image as a sequence of 28 row vectors.\n",
    "hidden_dim = 128\n",
    "classes_amount = 10 # 10 digits\n",
    "\n",
    "class basicLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidde_dim):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        # Parameters we want to train, i = input, h = hidden, f = forget gate, o = output, g = cell candidate to update cell\n",
    "        # because we have so many variables in our class, we are just going to stick to the classic activations and not have them as custom ones\n",
    "        self.W_ii = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_hi = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.W_if = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_hf = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.W_ig = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_hg = nn.Parameter(torch.Tensor(hidden_dim,hidden_dim))\n",
    "        self.b_g = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "        \n",
    "        self.W_io = nn.Parameter(torch.Tensor(input_dim, hidden_dim))\n",
    "        self.W_ho = nn.Parameter(torch.Tensor(hidden_dim, hidden_dim))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_dim))\n",
    "    #h_t = None, c_t = None\n",
    "    def forward(self, x):\n",
    "        #shape of x is supposed to be (batch, seq_length, input_dim)\n",
    "        batch, seq_length, _ = x.size()\n",
    "        \n",
    "        h_t = torch.zeros(batch, self.hidden_dim) \n",
    "        c_t = torch.zeros(batch, self.hidden_dim)\n",
    "    \n",
    "        #now, similar to the RNN implementation, we are going to go through the cell sequentially\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            x_t = x[:, t, :] # extract t-th timestep \n",
    "            \n",
    "            \n",
    "            #input gate, since x_t, y_t etc. are row vectors (per batch) we multiply them from the left\n",
    "            i_t = torch.sigmoid( x_t @ self.W_ii +  h_t @ self.W_hi+  self.b_i) # using torch.sigmoid instea of the nn.Sigmoid class since we do not have it as a variable anyway\n",
    "            #forget gate\n",
    "            f_t = torch.sigmoid(x_t @ self.W_if + h_t @ self.W_hf + self.b_f)\n",
    "            #cell candidate for update\n",
    "            g_t = torch.tanh(x_t @ self.W_ig + h_t @ self.W_hg + self.b_g)\n",
    "            #output\n",
    "            o_t = torch.sigmoid(x_t @ self.W_io +h_t @ self.W_ho + self.b_o)\n",
    "            \n",
    "            c_t = f_t * c_t + i_t * g_t #update cell\n",
    "            \n",
    "            # Update hidden state\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "        return h_t\n",
    "        \n",
    "#above we had the basic LSTM, now, lets get the classification working\n",
    "class MNISTLSMT(nn.Module):\n",
    "    def __init__(self, input_dim, hidde_dim, classes_amount):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = basicLSTM(input_dim, hidde_dim)\n",
    "        self.lstm_out = nn.Linear(hidde_dim, classes_amount)\n",
    "        self.act_out = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1,28,28)\n",
    "        out = self.lstm(x)\n",
    "        return self.act_out(self.lstm_out(out))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss at Batch 0 is 2.302553653717041\n",
      "Accuracy for Batch at 0 is 11.00%\n",
      "The loss at Batch 100 is 1.7761597633361816\n",
      "Accuracy for Batch at 100 is 47.00%\n",
      "The loss at Batch 200 is 1.7102913856506348\n",
      "Accuracy for Batch at 200 is 57.00%\n",
      "The loss at Batch 300 is 1.6082502603530884\n",
      "Accuracy for Batch at 300 is 72.00%\n",
      "The loss at Batch 400 is 1.5405768156051636\n",
      "Accuracy for Batch at 400 is 86.00%\n",
      "The loss at Batch 500 is 1.5157616138458252\n",
      "Accuracy for Batch at 500 is 82.00%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.01\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True, \n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False, \n",
    "    transform=transforms.ToTensor(),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "#for the batching, we could also use Dataloader, but for now lets keep it step by step and implement out own \"dataloader\"\n",
    "train_labels = train_dataset.targets\n",
    "train_data = train_dataset.data\n",
    "#normalize\n",
    "train_data = train_data.type(torch.float32)\n",
    "train_data = (train_data-torch.mean(train_data))/torch.std(train_data)\n",
    "\n",
    "#batching\n",
    "train_labels = train_labels.reshape(60000//batch_size, batch_size)\n",
    "train_data = train_data.reshape(60000//batch_size, batch_size, 28,28)\n",
    "\n",
    "model = MNISTLSMT(input_dim, hidden_dim, classes_amount)\n",
    "lossfunction = nn.CrossEntropyLoss() #classification loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(train_data.size()[0]):\n",
    "    images = train_data[i]\n",
    "    labels = train_labels[i]\n",
    "        \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = lossfunction(outputs, labels)\n",
    "    \n",
    "    _, predicted = torch.max(outputs, 1)  \n",
    "    total_correct = (predicted == labels).sum().item()\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 100 == 0: \n",
    "        print(f\"The loss for Batch {i} is {loss}\")\n",
    "        print(f\"Accuracy for Batch {i} is {total_correct/batch_size*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got a loss of 1.543831467628479 on our test set\n",
      "The accuracy on our test set is 84.71\n"
     ]
    }
   ],
   "source": [
    "#Lets test our Model\n",
    "test_data = test_dataset.data\n",
    "test_data = test_data.type(torch.float32).unsqueeze(-1)\n",
    "test_data = (test_data-torch.mean(test_data))/torch.std(test_data)\n",
    "test_labels = test_dataset.targets\n",
    "\n",
    "out = model(test_data)\n",
    "\n",
    "loss = lossfunction(out,test_labels)\n",
    "_, predicted = torch.max(out, 1)  \n",
    "total_correct = (predicted == test_labels).sum().item()\n",
    "\n",
    "print(f\"We got a loss of {loss.item()} on our test set\")\n",
    "print(f\"The accuracy on our test set is {total_correct /len(test_data) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MNIST classification LSTM.png'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_dot(out.mean(), params=dict(model.named_parameters())).render(\"MNIST classification LSTM\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
